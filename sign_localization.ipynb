{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import nms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2 as T\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import utils1\n",
    "from utils1 import Sign \n",
    "from evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path_GTSDB = \"GTSDBDataset\"\n",
    "annotaions_path_GTSDB = \"GTSDBDataset/gt.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSDBDataset(Dataset):\n",
    "    def __init__(self, images_path: str, annotaions_path: str):\n",
    "        num_examples = 900\n",
    "        self.examples = {}\n",
    "        lines = []\n",
    "        with open(annotaions_path) as f:\n",
    "            for line in f:\n",
    "                lines.append(line)\n",
    "        for sample_num in range(num_examples):\n",
    "            begins_with = str(sample_num).zfill(5)\n",
    "            annotations = [line for line in lines if line.startswith(begins_with)]\n",
    "            if len(annotations) == 0:\n",
    "                path = images_path + \"/\" + str(sample_num).zfill(5) + \".ppm\"\n",
    "                self.examples[path] = {\"image_path\": path, \"signs\": [Sign(0, 0, 0, 0, 0)]}\n",
    "            else:\n",
    "                for line in annotations:\n",
    "                    path, sign = self.create_sign(line, images_path)\n",
    "                    if path in self.examples:\n",
    "                        self.examples[path][\"signs\"].append(sign)\n",
    "                    else:\n",
    "                        self.examples[path] = {\"image_path\": path, \"signs\": [sign]}\n",
    "\n",
    "        self.examples = list(self.examples.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        data = self.examples[i]\n",
    "        image = cv2.imread(data[\"image_path\"])\n",
    "        image = T.ToTensor()(image)\n",
    "        image = T.ToDtype(torch.float32, scale=True)(image)\n",
    "        image = T.ToPureTensor()(image)\n",
    "        boxes = [[sign.topLeftX, sign.topLeftY, sign.bottomRightX, sign.bottomRightY] for sign in data[\"signs\"]]\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        #CHANGE IF YOU WANT CLASSES PREDICTED OR JUST LOCATIONS\n",
    "        labels = [sign.name for sign in data[\"signs\"]]\n",
    "        #labels = [1 for sign in data[\"signs\"]]\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        #If there is no sign in the image\n",
    "        if data[\"signs\"][0].name == 0: \n",
    "            target = {\"boxes\": torch.empty(0, 4), \"labels\": torch.tensor([0], dtype=torch.int64)}\n",
    "            \n",
    "        return image, target, data[\"image_path\"]\n",
    "    \n",
    "    def create_sign(self, line: str, images_path: str):\n",
    "        split = line.split(\";\")\n",
    "        image_path = images_path + \"/\" + split[0]\n",
    "        sign = Sign(float(split[3]), float(split[4]), float(split[1]), float(split[2]), int(split[5]) + 1)\n",
    "        return image_path, sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataset_GTSDB = GTSDBDataset(images_path_GTSDB, annotaions_path_GTSDB)\n",
    "dataloader_GTSDB = DataLoader(dataset_GTSDB, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_indices = list(range(600))\n",
    "test_indices = list(range(600, 900))\n",
    "\n",
    "train_dataset = Subset(dataset_GTSDB, train_indices)\n",
    "test_dataset = Subset(dataset_GTSDB, test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#CHANGE IF YOU WANT CLASSES PREDICTED OR JUST LOCATIONS\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 44)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr = 0.01,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 0.0005\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size = 5,\n",
    "    gamma = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "lrs = []\n",
    "dl = train_dataloader\n",
    "model.train()\n",
    "for i, epoch in enumerate(range(epochs)):\n",
    "    lr_scheduler = None\n",
    "    bar = tqdm(dl,total=len(dl))\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = len(dl) / 2\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for step, batch in enumerate(bar):\n",
    "        images = [image.to(device) for image in batch[0]]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in batch[1]]\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            loss = losses.item()\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            stats.append(loss)\n",
    "            lrs.append(lr)\n",
    "            bar.set_description(f\"Epoch {i}, Loss: {loss:.4f}, Lr: {lr:.4f}\")\n",
    "    scheduler.step()\n",
    "    torch.save(model, f\"Models/modelGTSDB_WithClasses{i}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats)\n",
    "plt.show()\n",
    "plt.plot(lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Models\\modelGTSDB_WithClasses9.pth\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signs(image, boxes):\n",
    "    images = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = math.floor(box[0].item()), math.floor(box[1].item()), math.ceil(box[2].item()), math.ceil(box[3].item())\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "        images.append(cropped_image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for data in test_dataloader:\n",
    "    for i in range(4):\n",
    "        image = data[0][i].to(device)\n",
    "        image_to_show = cv2.imread(data[2][i])\n",
    "        outputs = model([image])\n",
    "        boxes = outputs[0][\"boxes\"].detach().cpu()\n",
    "        scores = outputs[0][\"scores\"].detach().cpu()\n",
    "        labels = outputs[0][\"labels\"].detach().cpu()\n",
    "\n",
    "        # Apply non-maximum suppression\n",
    "        to_keep = nms(boxes, scores, 0.2)\n",
    "        boxes = boxes[to_keep]\n",
    "        scores = scores[to_keep]\n",
    "\n",
    "        for box, score, label1 in zip(boxes, scores, labels):\n",
    "            if score >= 0.4:  \n",
    "                # Draw the bounding box on the image\n",
    "                cv2.rectangle(image_to_show, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "                \n",
    "                #big red dot at x=50, y=50\n",
    "                cv2.circle(image_to_show, (50, 50), 10, (0, 0, 255), 8)\n",
    "\n",
    "                label = f'{score:.2f} {utils1.class_map[label1.item()]}'\n",
    "                \n",
    "                # Choose a font and get the text size\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.5\n",
    "                font_thickness = 1\n",
    "                text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]\n",
    "                \n",
    "                # Position the text at the top-left corner of the bounding box\n",
    "                text_x = int(box[0]) - 1\n",
    "                text_y = int(box[1])\n",
    "                \n",
    "                # Draw the text background rectangle\n",
    "                cv2.rectangle(image_to_show, (text_x, text_y - text_size[1] - 2), (text_x + text_size[0], text_y), (0, 255, 0), cv2.FILLED)\n",
    "                \n",
    "                # Put the text on the image\n",
    "                cv2.putText(image_to_show, label, (text_x, text_y - 2), font, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow(\"Image\", image_to_show)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        extract_signs(image_to_show, boxes)\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameter count of model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

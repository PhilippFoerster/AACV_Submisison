{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import nms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2 as T\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import utils1\n",
    "from utils1 import * \n",
    "from evaluation import evaluate_with_results\n",
    "from datasets import *\n",
    "import torchvision.transforms as transforms\n",
    "from visualisation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Color Ranges\n",
    "colors = {\n",
    "    \"red\": (([0, 50, 50], [15, 255, 255]), ([165, 50, 50], [180, 255, 255])),\n",
    "    \"blue\": (([90, 50, 50], [130, 255, 255]), None),\n",
    "    \"yellow\": (([10, 50, 50], [30, 255, 255]), None),\n",
    "}\n",
    "\n",
    "penalty = {\n",
    "    \"red\": 5,\n",
    "    \"blue\": 7,\n",
    "    \"yellow\": 50,\n",
    "}\n",
    "\n",
    "def get_viable_classes(image):\n",
    "    mask = np.zeros((30, 30, 3), dtype=np.uint8)\n",
    "    cv2.circle(mask, (15, 15), 15, (1, 1, 1), thickness=-1)\n",
    "    masked_img = mask * image\n",
    "\n",
    "    image_hsv = cv2.cvtColor(masked_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    innerMask = np.zeros((30, 30, 1), dtype=np.uint8)\n",
    "    cv2.rectangle(innerMask, (5, 5), (25, 25), (1), -1)\n",
    "    \n",
    "    #special case yellow\n",
    "    max = 0\n",
    "    dominantColor = \"\"\n",
    "    for color in colors:\n",
    "        colorMask = cv2.inRange(image_hsv, np.array(colors[color][0][0]), np.array(colors[color][0][1]))\n",
    "        maskYellow = np.zeros((30, 30, 1), dtype=np.uint8)\n",
    "        cv2.rectangle(maskYellow, (10,10), (20,20), (1), -1)\n",
    "        colorMask = cv2.multiply(colorMask, maskYellow)\n",
    "        if colorMask.sum() > max and (colorMask.sum()/255) >= 50:\n",
    "            max = colorMask.sum()\n",
    "            dominantColor = color\n",
    "    if dominantColor == \"yellow\":\n",
    "        return (color_map[\"yellow\"], penalty[\"yellow\"], dominantColor)\n",
    "    # if dominantColor == \"red\":\n",
    "    #     return (color_map[\"red\"], 50, dominantColor)\n",
    "    \n",
    "\n",
    "    max = 0\n",
    "    dominantColor = \"\"\n",
    "    for color in colors:\n",
    "        if color == \"yellow\":\n",
    "            continue\n",
    "        colorMask = cv2.inRange(image_hsv, np.array(colors[color][0][0]), np.array(colors[color][0][1]))\n",
    "        if colors[color][1]: \n",
    "            colorMask = cv2.add(colorMask, cv2.inRange(image_hsv, np.array(colors[color][1][0]), np.array(colors[color][1][1])))\n",
    "        if color == \"blue\":\n",
    "            colorMask = cv2.multiply(colorMask, innerMask)\n",
    "        if colorMask.sum() > max and (colorMask.sum()/255) >= 225:\n",
    "            max = colorMask.sum()\n",
    "            dominantColor = color\n",
    "    if dominantColor == \"\":\n",
    "        return (list(range(1, 43)), 0, dominantColor)\n",
    "    return (color_map[dominantColor], penalty[dominantColor], dominantColor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#Settings\n",
    "images_path_GTSDB = \"GTSDBDataset\"\n",
    "annotaions_path_GTSDB = \"GTSDBDataset/gt.txt\"\n",
    "localization_model_path = \"Models/modelGTSDB_WithoutClasses19.pth\"\n",
    "classification_model_path = \"Models/classification_new_30.pth\"\n",
    "batch_size = 4\n",
    "nms_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_model = torch.load(localization_model_path)\n",
    "classification_model = torch.load(classification_model_path)\n",
    "localization_model.to(device)\n",
    "classification_model.to(device)\n",
    "localization_model.eval()\n",
    "classification_model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_GTSDB = GTSDBDataset(True, images_path_GTSDB, annotaions_path_GTSDB)\n",
    "dataloader_GTSDB = DataLoader(dataset_GTSDB, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_indices = list(range(600, 900))\n",
    "test_dataset = Subset(dataset_GTSDB, test_indices)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = {}\n",
    "predictions = {}\n",
    "idx = 0\n",
    "\n",
    "for data in tqdm(test_dataloader):\n",
    "    for i in range(batch_size):\n",
    "        image = data[0][i].to(device)\n",
    "        image_to_show = cv2.imread(data[2][i])\n",
    "\n",
    "        outputs = localization_model([image])\n",
    "        boxes = outputs[0][\"boxes\"].detach().cpu()\n",
    "        scores = outputs[0][\"scores\"].detach().cpu()\n",
    "        labels = outputs[0][\"labels\"].detach().cpu()\n",
    "        \n",
    "        to_keep = nms(boxes, scores, nms_threshold)\n",
    "        boxes = boxes[to_keep]\n",
    "        scores = scores[to_keep]\n",
    "\n",
    "        images = extract_signs(image_to_show, boxes)\n",
    "        predictions[idx] = []\n",
    "        actual[idx] = []\n",
    "        visualColors = {}\n",
    "        for i2, box in enumerate(data[1][i][\"boxes\"]):\n",
    "            sign = Sign(box[2], box[3], box[0], box[1], class_map[data[1][i][\"labels\"][i2].item()])\n",
    "            actual[idx].append(sign)\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            image = cv2.resize(image, (30, 30))\n",
    "            viable_classes = get_viable_classes(image)\n",
    "            image = transform(image)\n",
    "            image = image.to(device)\n",
    "            output = classification_model(image)\n",
    "            for a in range(len(output[0])):\n",
    "                if a not in viable_classes[0]:\n",
    "                    output[0][a] -= viable_classes[1]\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            box = boxes[i]\n",
    "            sign = Sign(box[2], box[3], box[0], box[1], class_map[predicted.item()])\n",
    "            predictions[idx].append(sign)\n",
    "            visualColors[i] = viable_classes[2]\n",
    "        #show_image_with_signs(image_to_show, predictions[idx], visualColors)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_with_results(actual, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
